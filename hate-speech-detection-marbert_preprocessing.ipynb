{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fdb7c14",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:49.805617Z",
     "iopub.status.busy": "2024-12-27T01:28:49.805309Z",
     "iopub.status.idle": "2024-12-27T01:28:50.525651Z",
     "shell.execute_reply": "2024-12-27T01:28:50.524498Z"
    },
    "papermill": {
     "duration": 0.731993,
     "end_time": "2024-12-27T01:28:50.527174",
     "exception": false,
     "start_time": "2024-12-27T01:28:49.795181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/darija-hate-speech-detection/test_data_cleaned.csv\n",
      "/kaggle/input/darija-hate-speech-detection/df_without_Latin words.csv\n",
      "/kaggle/input/darija-hate-speech-detection/df_without_spaces.csv\n",
      "/kaggle/input/darija-hate-speech-detection/val_data_cleaned.csv\n",
      "/kaggle/input/darija-hate-speech-detection/df_after_normalisation.csv\n",
      "/kaggle/input/darija-hate-speech-detection/train.csv\n",
      "/kaggle/input/darija-hate-speech-detection/test.csv\n",
      "/kaggle/input/darija-hate-speech-detection/train_data_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5cfb91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.544295Z",
     "iopub.status.busy": "2024-12-27T01:28:50.543984Z",
     "iopub.status.idle": "2024-12-27T01:28:50.611137Z",
     "shell.execute_reply": "2024-12-27T01:28:50.610286Z"
    },
    "papermill": {
     "duration": 0.076673,
     "end_time": "2024-12-27T01:28:50.612385",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.535712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6419, 3)\n",
      "(1605, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = \"/kaggle/input/darija-hate-speech-detection/train.csv\"\n",
    "df_train = pd.read_csv(file_path)\n",
    "df_test = pd.read_csv('/kaggle/input/darija-hate-speech-detection/test.csv')\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1ec75d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.629548Z",
     "iopub.status.busy": "2024-12-27T01:28:50.629333Z",
     "iopub.status.idle": "2024-12-27T01:28:50.644609Z",
     "shell.execute_reply": "2024-12-27T01:28:50.643804Z"
    },
    "papermill": {
     "duration": 0.025133,
     "end_time": "2024-12-27T01:28:50.645969",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.620836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train=df_train.drop(columns=['Unnamed: 0'])\n",
    "df_test=df_test.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8abbbb18",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.662200Z",
     "iopub.status.busy": "2024-12-27T01:28:50.661953Z",
     "iopub.status.idle": "2024-12-27T01:28:50.669761Z",
     "shell.execute_reply": "2024-12-27T01:28:50.668999Z"
    },
    "papermill": {
     "duration": 0.01731,
     "end_time": "2024-12-27T01:28:50.671028",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.653718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Dataset Shape: (8024, 2)\n",
      "                                             comment  off\n",
      "0  ÙÙ†Ø§Ù†ÙŠÙ† Ø§Ù„ÙƒØ¨Øª ÙˆØ§Ù„ÙØ³Ø§Ø¯ .Ø¹Ù‚Ù„ÙŠØ© Ø¬Ù†Ø³ÙŠØ© Ù„Ø§ ØºÙŠØ±. Ø§Ù„Ø¹Ù...    1\n",
      "1  Ø§Ù„Ø¯Ø¹Ø§Ø±Ø© Ù‡Ø±Ø¨Øª Ù…Ù†Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠØ© Ùˆ Ø³ÙƒÙ†Øª ÙÙŠ Ø¨ÙˆØ²Ù†ÙŠÙ‚...    1\n",
      "2                       ÙƒÙˆÙ† ØºÙŠØ± Ø®Ø±ÙŠØªÙŠ Ùˆ Ù…Ø¯Ø±ØªÙŠØ´ Ù‡Ø§Ø¯Ø´ÙŠ    1\n",
      "3  Ù„Ø§ Ø­ÙˆÙ„ ÙˆÙ„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ø¹Ø¸ÙŠÙ… Ù„Ø§ Ø­ÙˆÙ„ Ùˆ...    0\n",
      "4  Ø§Ù„Ù„Ù‡ ÙŠØ±Ø­Ù… Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„ ÙƒØ¨ÙŠÙŠÙŠÙŠØ± Ùˆ Ù…Ø­ØªØ§Ø¬Ø© ...    0\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the datasets\n",
    "df = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "# Check the shape of the combined dataset\n",
    "print(f\"Combined Dataset Shape: {df.shape}\")\n",
    "\n",
    "# Display the first few rows of the combined dataset\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e687635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.687892Z",
     "iopub.status.busy": "2024-12-27T01:28:50.687680Z",
     "iopub.status.idle": "2024-12-27T01:28:50.698107Z",
     "shell.execute_reply": "2024-12-27T01:28:50.697365Z"
    },
    "papermill": {
     "duration": 0.01999,
     "end_time": "2024-12-27T01:28:50.699280",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.679290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "off\n",
       "1    4304\n",
       "0    3720\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['off'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e963c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.716946Z",
     "iopub.status.busy": "2024-12-27T01:28:50.716736Z",
     "iopub.status.idle": "2024-12-27T01:28:50.823289Z",
     "shell.execute_reply": "2024-12-27T01:28:50.822720Z"
    },
    "papermill": {
     "duration": 0.117554,
     "end_time": "2024-12-27T01:28:50.824825",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.707271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_emojis(text):\n",
    "    return [char for char in text if char in emoji.EMOJI_DATA]\n",
    "\n",
    "# Extract emojis for df\n",
    "emojis = df['comment'].apply(extract_emojis)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f81e24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.842212Z",
     "iopub.status.busy": "2024-12-27T01:28:50.841991Z",
     "iopub.status.idle": "2024-12-27T01:28:50.847817Z",
     "shell.execute_reply": "2024-12-27T01:28:50.847054Z"
    },
    "papermill": {
     "duration": 0.015619,
     "end_time": "2024-12-27T01:28:50.849083",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.833464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Emojis: {'ðŸ™', 'ðŸ™ˆ', 'ðŸ’¨', 'ðŸ™ƒ', 'ðŸ™Š', 'ðŸ’–', 'ðŸ¼', 'ðŸ’ƒ', 'ðŸ‘º', 'ðŸ˜', 'ðŸ˜‡', 'âœ´', 'ðŸŽ§', 'ðŸ™‚', 'ðŸ˜¿', 'ðŸ˜ƒ', 'ðŸ»', 'ðŸŒ', 'ðŸ˜Œ', 'ðŸ‘Ž', 'ðŸ˜¥', 'ðŸ‘¨', 'ðŸ™‡', 'ðŸ¤­', 'ðŸ˜³', 'ðŸ’„', 'ðŸ¤›', 'ðŸ ', 'ðŸ˜‹', 'ðŸ¨', 'âœ…', 'ðŸ˜Ÿ', 'ðŸ¦', 'ðŸ­', 'ðŸ¡', 'ðŸŒ¹', 'ðŸ’•', 'ðŸ§¡', 'ðŸ¤Ÿ', 'ðŸ˜Ž', 'ðŸ˜ž', 'ðŸ˜¡', 'ðŸ’', 'ðŸ’µ', 'ðŸ‘', 'ðŸ˜–', 'ðŸš«', 'ðŸ˜‚', 'ðŸ§', 'ðŸ–¤', 'ðŸ¼', 'ðŸ‘€', 'ðŸ˜¶', 'ðŸ˜', 'ðŸ¿', 'ðŸ¤¢', 'ðŸ¤“', 'ðŸ™', 'ðŸ‘‰', 'ðŸ¤ ', 'ðŸ•µ', 'ðŸ’', 'ðŸ¤š', 'â¤', 'ðŸ‘†', 'ðŸ’”', 'ðŸ‘¦', 'ðŸ¸', 'ðŸ’¯', 'ðŸ’¸', 'ðŸ¤®', 'ðŸ¬', 'ðŸ‘§', 'ðŸ¥´', 'ðŸ•·', 'â˜º', 'ðŸ¤¯', 'ðŸ‹', 'ðŸ˜¤', 'ðŸ’—', 'ðŸ˜œ', 'ðŸ•‹', 'â™¥', 'ðŸ˜´', 'ðŸ“', 'ðŸ›‘', 'ðŸ˜„', 'â¬‡', 'ðŸ˜', 'ðŸŒ„', 'ðŸ’©', 'ðŸ˜¨', 'ðŸŒ¸', 'ðŸ‡', 'ðŸŽ', 'ðŸ’š', 'ðŸ‰', 'ðŸŽ', 'ðŸ…', 'ðŸŒš', 'ðŸ³', 'ðŸ¤‘', 'ðŸ˜¢', 'ðŸ¤¦', 'ðŸŒº', 'ðŸ”‡', 'ðŸ’ª', 'ðŸ’»', 'ðŸ”«', 'ðŸƒ', 'ðŸ»', 'ðŸ˜¯', 'ðŸš¨', 'ðŸ™Œ', 'ðŸŒ±', 'ðŸ’£', 'ðŸ’', 'ðŸ‘ˆ', 'ðŸ’¡', 'ðŸ˜…', 'ðŸ’…', 'ðŸ’Œ', 'ðŸ˜½', 'â™‚', 'ðŸ¤©', 'âœ”', 'ðŸ—', 'ðŸ’˜', 'ðŸ’‹', 'ðŸ¶', 'ðŸ˜«', 'âœŒ', 'ðŸ¦…', 'ðŸ¤', 'ðŸ‘', 'ðŸš¾', 'ðŸŒ·', 'ðŸŽ²', 'ðŸ§', 'ðŸ˜’', 'ðŸ˜‘', 'ðŸ˜·', 'ðŸ¥€', 'ðŸ˜¹', 'âœ‹', 'ðŸ˜ª', 'ðŸ˜ˆ', 'ðŸ¤£', 'ðŸ™„', 'ðŸ‘‡', 'ðŸš¶', 'ðŸ˜²', 'ðŸ’“', 'ðŸ””', 'ðŸ“²', 'ðŸ˜­', 'ðŸ¥º', 'ðŸ”¥', 'ðŸŒ»', 'ðŸ¥–', 'ðŸ§', 'ðŸ˜“', 'ðŸ˜©', 'ðŸ™†', 'ðŸ', 'ðŸŽ‚', 'ðŸœ', 'âŒ', 'ðŸ˜§', 'ðŸŽ‰', 'âš¡', 'ðŸŒ™', 'ðŸ¥°', 'ðŸ‘‘', 'ðŸ½', 'ðŸ¤™', 'ðŸ¤¨', 'ðŸ˜¦', 'ðŸ¤§', 'ðŸ–•', 'ðŸ”’', 'ðŸ¯', 'ðŸ’™', 'ðŸ«', 'ðŸ¤”', 'ðŸ˜ ', 'ðŸ¤ª', 'ðŸ”ª', 'ðŸ˜”', 'ðŸ˜Š', 'ðŸ•', 'ðŸ¤¤', 'ðŸ˜˜', 'â˜', 'ðŸ˜š', 'ðŸ˜±', 'ðŸ˜£', 'ðŸ’Ÿ', 'ðŸ¤—', 'ðŸ·', 'ðŸ˜€', 'ðŸ¤²', 'ðŸ¯', 'ðŸ˜', 'ðŸ€', 'ðŸ’¥', 'ðŸ’ž', 'ðŸŒˆ', 'ðŸ¥Š', 'ðŸ™‹', 'ðŸ˜¬', 'ðŸ¤¡', 'ðŸ˜†', 'ðŸ–', 'ðŸ¥µ', 'ðŸ˜•', 'ðŸ’', 'ðŸ™‰', 'ðŸ’›', 'ðŸ‘Š', 'ðŸ‘ª', 'ðŸŒž', 'ðŸ‘Œ', 'ðŸ˜°', 'ðŸ†', 'ðŸ‘‹', 'â›”', 'ðŸ”´', 'ðŸµ', 'ðŸ°', 'ðŸ­', 'ðŸŒ', 'â™€', 'ðŸ˜µ', 'ðŸ‘¶', 'ðŸ˜', 'ðŸ¤¬', 'ðŸ‘¿', 'ðŸ’°', 'ðŸ¤•', 'ðŸ’œ', 'ðŸ˜‰', 'ðŸ‘©', 'ðŸ§ '}\n",
      "Total Unique Emojis: 244\n"
     ]
    }
   ],
   "source": [
    "emojis\n",
    "# Combine all emojis into a single list from both datasets\n",
    "all_emojis = set(\n",
    "    [emo for emolist in emojis for emo in emolist] \n",
    ")\n",
    "\n",
    "# Display all unique emojis\n",
    "print(\"Unique Emojis:\", all_emojis)\n",
    "print(\"Total Unique Emojis:\", len(all_emojis))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "040acb79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.866414Z",
     "iopub.status.busy": "2024-12-27T01:28:50.866212Z",
     "iopub.status.idle": "2024-12-27T01:28:50.872509Z",
     "shell.execute_reply": "2024-12-27T01:28:50.871685Z"
    },
    "papermill": {
     "duration": 0.016409,
     "end_time": "2024-12-27T01:28:50.873826",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.857417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Emoji\n",
      "107     ðŸ’»\n",
      "108     ðŸ”«\n",
      "109     ðŸƒ\n",
      "110     ðŸ»\n",
      "111     ðŸ˜¯\n",
      "..    ...\n",
      "239     ðŸ¤•\n",
      "240     ðŸ’œ\n",
      "241     ðŸ˜‰\n",
      "242     ðŸ‘©\n",
      "243     ðŸ§ \n",
      "\n",
      "[137 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with unique emojis\n",
    "emoji_df = pd.DataFrame({\"Emoji\": list(all_emojis)})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(emoji_df[107::])  # Display the first 10 emojis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9834ab0",
   "metadata": {
    "papermill": {
     "duration": 0.007846,
     "end_time": "2024-12-27T01:28:50.890288",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.882442",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# remove emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33036c2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:50.907093Z",
     "iopub.status.busy": "2024-12-27T01:28:50.906883Z",
     "iopub.status.idle": "2024-12-27T01:28:51.025851Z",
     "shell.execute_reply": "2024-12-27T01:28:51.024878Z"
    },
    "papermill": {
     "duration": 0.128638,
     "end_time": "2024-12-27T01:28:51.027161",
     "exception": false,
     "start_time": "2024-12-27T01:28:50.898523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             comment  off\n",
      "0  ÙÙ†Ø§Ù†ÙŠÙ† Ø§Ù„ÙƒØ¨Øª ÙˆØ§Ù„ÙØ³Ø§Ø¯ .Ø¹Ù‚Ù„ÙŠØ© Ø¬Ù†Ø³ÙŠØ© Ù„Ø§ ØºÙŠØ±. Ø§Ù„Ø¹Ù...    1\n",
      "1  Ø§Ù„Ø¯Ø¹Ø§Ø±Ø© Ù‡Ø±Ø¨Øª Ù…Ù†Ù‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø­Ù…Ø¯ÙŠØ© Ùˆ Ø³ÙƒÙ†Øª ÙÙŠ Ø¨ÙˆØ²Ù†ÙŠÙ‚...    1\n",
      "2                       ÙƒÙˆÙ† ØºÙŠØ± Ø®Ø±ÙŠØªÙŠ Ùˆ Ù…Ø¯Ø±ØªÙŠØ´ Ù‡Ø§Ø¯Ø´ÙŠ    1\n",
      "3  Ù„Ø§ Ø­ÙˆÙ„ ÙˆÙ„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡ Ø§Ù„Ø¹Ù„ÙŠ Ø§Ù„Ø¹Ø¸ÙŠÙ… Ù„Ø§ Ø­ÙˆÙ„ Ùˆ...    0\n",
      "4  Ø§Ù„Ù„Ù‡ ÙŠØ±Ø­Ù… Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† Ø¹Ù†Ø¯ÙŠ Ù…Ø´ÙƒÙ„ ÙƒØ¨ÙŠÙŠÙŠÙŠØ± Ùˆ Ù…Ø­ØªØ§Ø¬Ø© ...    0\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import pandas as pd\n",
    "\n",
    "# Function to remove emojis from text\n",
    "def remove_emojis(text):\n",
    "    return ''.join(char for char in text if char not in emoji.EMOJI_DATA)\n",
    "\n",
    "# Apply the function to the 'comment' column\n",
    "df['comment'] = df['comment'].apply(remove_emojis)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6aa00c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:28:51.044181Z",
     "iopub.status.busy": "2024-12-27T01:28:51.043976Z",
     "iopub.status.idle": "2024-12-27T01:29:59.790767Z",
     "shell.execute_reply": "2024-12-27T01:29:59.789154Z"
    },
    "papermill": {
     "duration": 68.765949,
     "end_time": "2024-12-27T01:29:59.801197",
     "exception": true,
     "start_time": "2024-12-27T01:28:51.035248",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like UBC-NLP/MARBERT is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNameResolutionError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wrap_proxy_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1094\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1095\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaierror\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNameResolutionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameResolutionError\u001b[0m: <urllib3.connection.HTTPSConnection object at 0x780b6d31a770>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    844\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_e\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mreason\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /UBC-NLP/MARBERT/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x780b6d31a770>: Failed to resolve 'huggingface.co' ([Errno -3] Temporary failure in name resolution)\"))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1751\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1752\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1753\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1675\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: (MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /UBC-NLP/MARBERT/resolve/main/config.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x780b6d31a770>: Failed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))'), '(Request ID: 91ec1f8f-e52b-45b0-b143-62f966409732)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    403\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1240\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m   1241\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0;31m# Otherwise, raise appropriate error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m         \u001b[0m_raise_on_head_call_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_call_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         \u001b[0;31m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1858\u001b[0;31m         raise LocalEntryNotFoundError(\n\u001b[0m\u001b[1;32m   1859\u001b[0m             \u001b[0;34m\"An error happened while trying to locate the file on the Hub and we cannot find the requested files\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-4404e2dc8762>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mMARBERT_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UBC-NLP/MARBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mMARBERT_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UBC-NLP/MARBERT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m                     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m                     config = AutoConfig.from_pretrained(\n\u001b[0m\u001b[1;32m    855\u001b[0m                         \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m                     )\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    974\u001b[0m         \u001b[0mcode_revision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"code_revision\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 976\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munused_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    977\u001b[0m         \u001b[0mhas_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0mhas_local_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0moriginal_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;31m# Get config dict associated with the base config file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_commit_hash\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0moriginal_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_commit_hash\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36m_get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;31m# Load from local folder or from cache or download from model Hub and cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mconfiguration_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m         ):\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresolved_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         raise EnvironmentError(\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;34mf\"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this file, couldn't find it in the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;34mf\" cached files and it looks like {path_or_repo_id} is not the path to a directory containing a file named\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like UBC-NLP/MARBERT is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel,AutoModelForSequenceClassification\n",
    "\n",
    "MARBERT_tokenizer = AutoTokenizer.from_pretrained(\"UBC-NLP/MARBERT\")\n",
    "MARBERT_model = AutoModelForSequenceClassification.from_pretrained(\"UBC-NLP/MARBERT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9ef79",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009db56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:20:58.975356Z",
     "iopub.status.busy": "2024-12-25T23:20:58.974791Z",
     "iopub.status.idle": "2024-12-25T23:20:59.087027Z",
     "shell.execute_reply": "2024-12-25T23:20:59.086095Z",
     "shell.execute_reply.started": "2024-12-25T23:20:58.975325Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "# Function to remove punctuation from text\n",
    "def remove_punctuation(text):\n",
    "    return ''.join(char for char in text if char not in string.punctuation)\n",
    "\n",
    "# Apply the function to the 'comment' column\n",
    "df['comment'] = df['comment'].apply(remove_punctuation)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060e4df5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Single digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85d2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:26:15.282896Z",
     "iopub.status.busy": "2024-12-25T23:26:15.282561Z",
     "iopub.status.idle": "2024-12-25T23:26:15.333113Z",
     "shell.execute_reply": "2024-12-25T23:26:15.332176Z",
     "shell.execute_reply.started": "2024-12-25T23:26:15.282845Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to detect single-digit numbers in text\n",
    "def detect_single_digits(text):\n",
    "    return re.findall(r'\\b\\d\\b', text)\n",
    "\n",
    "# Create a new column to store detected single-digit numbers\n",
    "df['single_digits'] = df['comment'].apply(detect_single_digits)\n",
    "\n",
    "# Display rows with detected single-digit numbers\n",
    "detected_rows = df[df['single_digits'].apply(len) > 0]\n",
    "print(detected_rows[['comment', 'single_digits']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa2f87f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:27:32.386261Z",
     "iopub.status.busy": "2024-12-25T23:27:32.385941Z",
     "iopub.status.idle": "2024-12-25T23:27:32.419536Z",
     "shell.execute_reply": "2024-12-25T23:27:32.418589Z",
     "shell.execute_reply.started": "2024-12-25T23:27:32.386236Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to remove single-digit numbers from text\n",
    "def remove_single_digits(text):\n",
    "    return re.sub(r'\\b\\d\\b', '', text)\n",
    "\n",
    "# Remove single-digit numbers from the 'comment' column\n",
    "df['comment'] = df['comment'].apply(remove_single_digits)\n",
    "\n",
    "# Drop the 'single_digits' column (optional)\n",
    "df.drop(columns=['single_digits'], inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89598a5f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Latin-script words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc50523e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:31:17.871466Z",
     "iopub.status.busy": "2024-12-25T23:31:17.871089Z",
     "iopub.status.idle": "2024-12-25T23:31:17.907986Z",
     "shell.execute_reply": "2024-12-25T23:31:17.906962Z",
     "shell.execute_reply.started": "2024-12-25T23:31:17.871434Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to detect text primarily written in Latin characters\n",
    "def detect_latin_darija(text):\n",
    "    # Define a pattern for Latin characters and numbers\n",
    "    latin_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    return bool(latin_pattern.search(text))  # Return True if Latin characters are found\n",
    "\n",
    "# Apply the function to the 'comment' column and create a new column for detection\n",
    "df['is_latin_darija'] = df['comment'].apply(detect_latin_darija)\n",
    "\n",
    "# Display rows with detected Latin-script Darija\n",
    "latin_darija_rows = df[df['is_latin_darija']]\n",
    "print(latin_darija_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d436e6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:31:41.156163Z",
     "iopub.status.busy": "2024-12-25T23:31:41.155831Z",
     "iopub.status.idle": "2024-12-25T23:31:41.162801Z",
     "shell.execute_reply": "2024-12-25T23:31:41.161963Z",
     "shell.execute_reply.started": "2024-12-25T23:31:41.156136Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['is_latin_darija'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0ddf66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:35:50.827009Z",
     "iopub.status.busy": "2024-12-25T23:35:50.826662Z",
     "iopub.status.idle": "2024-12-25T23:35:50.832545Z",
     "shell.execute_reply": "2024-12-25T23:35:50.831733Z",
     "shell.execute_reply.started": "2024-12-25T23:35:50.826979Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns=['is_latin_darija'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015449b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:36:03.755625Z",
     "iopub.status.busy": "2024-12-25T23:36:03.755328Z",
     "iopub.status.idle": "2024-12-25T23:36:03.787347Z",
     "shell.execute_reply": "2024-12-25T23:36:03.786379Z",
     "shell.execute_reply.started": "2024-12-25T23:36:03.755600Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def detect_latin_darija(text):\n",
    "    latin_pattern = re.compile(r'[a-zA-Z]+')\n",
    "    return bool(latin_pattern.search(text))\n",
    "\n",
    "# Filter comments with transliterated Latin script\n",
    "latin_darija_rows = df[df['comment'].apply(detect_latin_darija)]\n",
    "print(\"Rows with Latin-script Darija:\")\n",
    "print(latin_darija_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66481fc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:37:55.878596Z",
     "iopub.status.busy": "2024-12-25T23:37:55.878300Z",
     "iopub.status.idle": "2024-12-25T23:37:55.917357Z",
     "shell.execute_reply": "2024-12-25T23:37:55.916434Z",
     "shell.execute_reply.started": "2024-12-25T23:37:55.878570Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to extract Latin-script words\n",
    "def extract_latin_words(text):\n",
    "    latin_pattern = re.compile(r'\\b[a-zA-Z]+\\b')  # Matches Latin script words only\n",
    "    return latin_pattern.findall(text)\n",
    "\n",
    "# Apply to the dataset\n",
    "df['latin_words'] = df['comment'].apply(extract_latin_words)\n",
    "\n",
    "# Display rows with extracted Latin words\n",
    "latin_words_df = df[df['latin_words'].apply(lambda x: len(x) > 0)]\n",
    "print(\"Comments with Latin-script words:\")\n",
    "print(latin_words_df[['comment', 'latin_words']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77508fe4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:38:26.988506Z",
     "iopub.status.busy": "2024-12-25T23:38:26.988207Z",
     "iopub.status.idle": "2024-12-25T23:38:26.998495Z",
     "shell.execute_reply": "2024-12-25T23:38:26.997527Z",
     "shell.execute_reply.started": "2024-12-25T23:38:26.988483Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of Latin words\n",
    "all_latin_words = [word for words in df['latin_words'] for word in words]\n",
    "\n",
    "# Count occurrences of each word\n",
    "latin_word_counts = Counter(all_latin_words)\n",
    "\n",
    "# Convert to a DataFrame for better visualization\n",
    "latin_word_counts_df = pd.DataFrame(latin_word_counts.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False)\n",
    "\n",
    "# Display the top Latin-script words\n",
    "print(\"Top Latin-script words:\")\n",
    "print(latin_word_counts_df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec52383",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:40:11.676735Z",
     "iopub.status.busy": "2024-12-25T23:40:11.676418Z",
     "iopub.status.idle": "2024-12-25T23:40:11.693041Z",
     "shell.execute_reply": "2024-12-25T23:40:11.692205Z",
     "shell.execute_reply.started": "2024-12-25T23:40:11.676710Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "latin_word_counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef28c8c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3712f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-25T23:43:00.729182Z",
     "iopub.status.busy": "2024-12-25T23:43:00.728847Z",
     "iopub.status.idle": "2024-12-25T23:43:00.737637Z",
     "shell.execute_reply": "2024-12-25T23:43:00.736604Z",
     "shell.execute_reply.started": "2024-12-25T23:43:00.729156Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save extracted Latin words to a CSV\n",
    "latin_word_counts_df.to_csv(\"latin_words_counts.csv\", index=False)\n",
    "print(\"Extracted Latin words saved to 'latin_words_counts.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c5193",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:03:39.323797Z",
     "iopub.status.busy": "2024-12-26T00:03:39.323360Z",
     "iopub.status.idle": "2024-12-26T00:03:39.329364Z",
     "shell.execute_reply": "2024-12-26T00:03:39.328439Z",
     "shell.execute_reply.started": "2024-12-26T00:03:39.323761Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.drop(columns=['latin_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82289f99",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402c7749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:03:43.587318Z",
     "iopub.status.busy": "2024-12-26T00:03:43.587003Z",
     "iopub.status.idle": "2024-12-26T00:03:43.596054Z",
     "shell.execute_reply": "2024-12-26T00:03:43.595269Z",
     "shell.execute_reply.started": "2024-12-26T00:03:43.587294Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transcription_dict = {\n",
    "    \"tv\": \"ØªÙ„ÙØ§Ø²\",\n",
    "    \"groupe\": \"Ù…Ø¬Ù…ÙˆØ¹Ø©\",\n",
    "    \"WhatsApp\": \"ÙˆØ§ØªØ³Ø§Ø¨\",\n",
    "    \"mon\": \"Ù…ÙˆÙ†\",\n",
    "    \"ce\": \"Ù‡Ø°Ø§\",\n",
    "    \"lien\": \"Ø±Ø§Ø¨Ø·\",\n",
    "    \"pour\": \"Ù„Ù€\",\n",
    "    \"bimo\": \"Ø¨ÙŠÙ…Ùˆ\",\n",
    "    \"TV\": \"ØªÙ„ÙØ§Ø²\",\n",
    "    \"Utilise\": \"ÙŠØ³ØªØ®Ø¯Ù…\",\n",
    "    \"BIMO\": \"Ø¨ÙŠÙ…Ùˆ\",\n",
    "    \"top\": \"Ø§Ù„Ø£Ø¹Ù„Ù‰\",\n",
    "    \"ni\": \"Ù†ÙŠ\",\n",
    "    \"adn\": \"Ø§Ù„Ø­Ù…Ø¶ Ø§Ù„Ù†ÙˆÙˆÙŠ\",\n",
    "    \"dh\": \"Ø¯Ø±Ù‡Ù…\",\n",
    "    \"flambo\": \"ÙÙ„Ø§Ù…Ø¨Ùˆ\",\n",
    "    \"bravo\": \"Ø¨Ø±Ø§ÙÙˆ\",\n",
    "    \"hicham\": \"Ù‡Ø´Ø§Ù…\",\n",
    "    \"jiji\": \"Ø¬ÙŠØ¬ÙŠ\",\n",
    "    \"respect\": \"Ø§Ø­ØªØ±Ø§Ù…\",\n",
    "    \"vivavideo\": \"ÙÙŠÙØ§ ÙÙŠØ¯ÙŠÙˆ\",\n",
    "    \"l\": \"Ù„Ù€\",\n",
    "    \"Venu\": \"Ø¬Ø§Ø¡\",\n",
    "    \"Dr\": \"Ø¯ÙƒØªÙˆØ±\",\n",
    "    \"Mafia\": \"Ù…Ø§ÙÙŠØ§\",\n",
    "    \"Beats\": \"Ù†Ø¨Ø¶Ø§Øª\",\n",
    "    \"commenter\": \"ØªØ¹Ù„ÙŠÙ‚\",\n",
    "    \"uhu\": \"Ø£ÙˆÙ‡\",\n",
    "    \"vue\": \"Ø±Ø¤ÙŠØ©\",\n",
    "    \"Hd\": \"Ø§ØªØ´ Ø¯ÙŠ\",\n",
    "    \"Tetris\": \"ØªØªØ±ÙŠØ³\",\n",
    "    \"trip\": \"Ø±Ø­Ù„Ø©\",\n",
    "    \"Moroco\": \"Ø§Ù„Ù…ØºØ±Ø¨\",\n",
    "    \"Web\": \"ÙˆÙŠØ¨\",\n",
    "    \"Chanel\": \"Ø´Ø§Ù†ÙŠÙ„\",\n",
    "    \"Sofia\": \"ØµÙˆÙÙŠØ§\",\n",
    "    \"balwad\": \"Ø¨Ø§Ù„ÙˆØ§Ø¯\",\n",
    "    \"gaiming\": \"Ø§Ù„Ù„Ø¹Ø¨\",\n",
    "    \"arte\": \"Ø¢Ø±Øª\",\n",
    "    \"un\": \"ÙˆØ§Ø­Ø¯\",\n",
    "    \"cest\": \"Ù‡Ø°Ø§\",\n",
    "    \"dislike\": \"Ø¹Ø¯Ù… Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨\",\n",
    "    \"Charles\": \"ØªØ´Ø§Ø±Ù„Ø²\",\n",
    "    \"James\": \"Ø¬ÙŠÙ…Ø³\",\n",
    "    \"moroccan\": \"Ù…ØºØ±Ø¨ÙŠ\",\n",
    "    \"Version\": \"Ù†Ø³Ø®Ø©\",\n",
    "    \"chof\": \"Ø´ÙˆÙ\",\n",
    "    \"brahim\": \"Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…\",\n",
    "    \"Toop\": \"Ø§Ù„Ø£ÙØ¶Ù„\",\n",
    "    \"love\": \"Ø­Ø¨\",\n",
    "    \"TRUE\": \"ØµØ­ÙŠØ­\",\n",
    "    \"b\": \"Ø¨Ù€\",\n",
    "    \"star\": \"Ù†Ø¬Ù…Ø©\",\n",
    "    \"Shit\": \"Ù„Ø¹Ù†Ø©\",\n",
    "    \"Level\": \"Ù…Ø³ØªÙˆÙ‰\",\n",
    "    \"CNSS\": \"ØµÙ†Ø¯ÙˆÙ‚ Ø§Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠ\",\n",
    "    \"sanae\": \"Ø³Ù†Ø§Ø¡\",\n",
    "    \"Tfooo\": \"ØªÙÙˆÙˆ\",\n",
    "    \"like\": \"Ø¥Ø¹Ø¬Ø§Ø¨\",\n",
    "    \"Hhhh\": \"Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"Lherba\": \"Ù„Ù‡Ø±Ø¨Ù‡\",\n",
    "    \"hhhhh\": \"Ù‡Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"live\": \"Ù…Ø¨Ø§Ø´Ø±\",\n",
    "    \"reacts\": \"Ø±Ø¯ÙˆØ¯\",\n",
    "    \"you\": \"Ø£Ù†Øª\",\n",
    "    \"ADN\": \"Ø§Ù„Ø­Ù…Ø¶ Ø§Ù„Ù†ÙˆÙˆÙŠ\",\n",
    "    \"airpods\": \"Ø¥ÙŠØ±Ø¨ÙˆØ¯Ø²\",\n",
    "    \"sinon\": \"Ø¥Ø°Ø§\",\n",
    "    \"lclip\": \"Ø§Ù„ÙƒÙ„Ø¨\",\n",
    "    \"Trand\": \"ØªØ±Ù†Ø¯\",\n",
    "    \"Tv\": \"ØªÙ„ÙØ§Ø²\",\n",
    "    \"shooting\": \"ØªØµÙˆÙŠØ±\",\n",
    "    \"hada\": \"Ù‡Ø°Ø§\",\n",
    "    \"pro\": \"Ù…Ø­ØªØ±Ù\",\n",
    "    \"per\": \"Ù„ÙƒÙ„\",\n",
    "    \"Abboner\": \"Ù…Ø´ØªØ±Ùƒ\",\n",
    "    \"artist\": \"ÙÙ†Ø§Ù†\",\n",
    "    \"mekup\": \"Ù…ÙƒÙŠØ§Ø¬\",\n",
    "    \"mariage\": \"Ø²ÙˆØ§Ø¬\",\n",
    "    \"acte\": \"ÙØ¹Ù„\",\n",
    "    \"les\": \"Ø§Ù„\",\n",
    "    \"w\": \"Ùˆ\",\n",
    "    \"conversastion\": \"Ù…Ø­Ø§Ø¯Ø«Ø©\",\n",
    "    \"Bimo\": \"Ø¨ÙŠÙ…Ùˆ\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bab4fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:04:25.317224Z",
     "iopub.status.busy": "2024-12-26T00:04:25.316916Z",
     "iopub.status.idle": "2024-12-26T00:04:25.359170Z",
     "shell.execute_reply": "2024-12-26T00:04:25.358367Z",
     "shell.execute_reply.started": "2024-12-26T00:04:25.317201Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def map_and_replace(comment, transcription_dict):\n",
    "    words = comment.split()\n",
    "    replaced_words = [transcription_dict.get(word, word) for word in words]\n",
    "    return ' '.join(replaced_words)\n",
    "\n",
    "# Apply the mapping to the 'comment' column\n",
    "df['comment'] = df['comment'].apply(lambda x: map_and_replace(x, transcription_dict))\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a613bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T00:17:54.510161Z",
     "iopub.status.busy": "2024-12-26T00:17:54.509824Z",
     "iopub.status.idle": "2024-12-26T00:17:54.539636Z",
     "shell.execute_reply": "2024-12-26T00:17:54.539008Z",
     "shell.execute_reply.started": "2024-12-26T00:17:54.510135Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save extracted Latin words to a CSV\n",
    "df.to_csv(\"df_without_Latin words.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7032c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T23:46:19.485903Z",
     "iopub.status.busy": "2024-12-26T23:46:19.485626Z",
     "iopub.status.idle": "2024-12-26T23:46:19.540230Z",
     "shell.execute_reply": "2024-12-26T23:46:19.539594Z",
     "shell.execute_reply.started": "2024-12-26T23:46:19.485882Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"/kaggle/input/darija-hate-speech-detection/df_without_Latin words.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c61952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T23:46:36.909418Z",
     "iopub.status.busy": "2024-12-26T23:46:36.909079Z",
     "iopub.status.idle": "2024-12-26T23:46:36.916388Z",
     "shell.execute_reply": "2024-12-26T23:46:36.915635Z",
     "shell.execute_reply.started": "2024-12-26T23:46:36.909390Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99642d1b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Normalize repeated letters except \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162daa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T23:56:09.377225Z",
     "iopub.status.busy": "2024-12-26T23:56:09.376939Z",
     "iopub.status.idle": "2024-12-26T23:56:09.401959Z",
     "shell.execute_reply": "2024-12-26T23:56:09.401125Z",
     "shell.execute_reply.started": "2024-12-26T23:56:09.377203Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import regex as re \n",
    "# Function to normalize repeated \"Ù‡Ù‡Ù‡\" to a consistent format\n",
    "def normalize_laughter(text):\n",
    "    # Replace repeated \"Ù‡\" (3 or more times) with a consistent \"Ù‡Ù‡\"\n",
    "    return re.sub(r'(Ù‡)\\1{2,}', r'\\1\\1', text)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ ÙˆØ´ ÙƒØªØ´ÙˆÙ Ø´ÙŠ ÙˆØ§Ø­Ø¯ ðŸ˜‚ðŸ˜‚\",\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"Ø´Ùˆ Ù‡Ù‡Ù‡Ù‡Ù‡ Ø´Ù†Ùˆ Ø§Ù„ÙˆØ¶Ø¹\",\n",
    "    \"Ø¹Ø§Ø¯ÙŠ Ø¬Ø¯Ø§ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø¯ÙˆÙ† Ù‡Ù‡Ù‡Ù‡Ù‡\"\n",
    "]\n",
    "\n",
    "# Apply normalization to the test cases\n",
    "normalized_test_cases = [normalize_laughter(case) for case in test_cases]\n",
    "\n",
    "# Display the original and normalized results\n",
    "for i, (original, normalized) in enumerate(zip(test_cases, normalized_test_cases)):\n",
    "    print(f\"Original Test Case {i+1}: {original}\")\n",
    "    print(f\"Normalized Test Case {i+1}: {normalized}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c1af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:01:27.005214Z",
     "iopub.status.busy": "2024-12-27T00:01:27.004911Z",
     "iopub.status.idle": "2024-12-27T00:01:27.065362Z",
     "shell.execute_reply": "2024-12-27T00:01:27.064520Z",
     "shell.execute_reply.started": "2024-12-27T00:01:27.005192Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "# Function to normalize repeated letters (except \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\")\n",
    "def normalize_repeated_letters_except_laughter(text):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    # Pattern to match any repeated characters except \"Ù‡\"\n",
    "    pattern = r'([^Ù‡])\\1+'\n",
    "    # Replace the repeated characters with a single occurrence\n",
    "    return re.sub(pattern, r'\\1', text)\n",
    "\n",
    "# Ensure all comments are strings before applying the function\n",
    "df['comment'] = df['comment'].astype(str)\n",
    "df['comment'] = df['comment'].apply(normalize_repeated_letters_except_laughter)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d3fbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T23:57:24.612031Z",
     "iopub.status.busy": "2024-12-26T23:57:24.611670Z",
     "iopub.status.idle": "2024-12-26T23:57:24.617332Z",
     "shell.execute_reply": "2024-12-26T23:57:24.616385Z",
     "shell.execute_reply.started": "2024-12-26T23:57:24.612001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test string\n",
    "s = \"ÙƒØ¨ÙŠÙŠÙŠÙŠØ± ØªÙÙˆÙˆÙˆÙˆ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\"\n",
    "# Apply the function\n",
    "result = normalize_repeated_letters_except_laughter(s)\n",
    "print(result)  # Output: \"ÙƒØ¨ÙŠØ± ØªÙÙˆ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370c46f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T23:58:39.655682Z",
     "iopub.status.busy": "2024-12-26T23:58:39.655357Z",
     "iopub.status.idle": "2024-12-26T23:58:39.668961Z",
     "shell.execute_reply": "2024-12-26T23:58:39.667969Z",
     "shell.execute_reply.started": "2024-12-26T23:58:39.655653Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958c295",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:02:49.332601Z",
     "iopub.status.busy": "2024-12-27T00:02:49.332244Z",
     "iopub.status.idle": "2024-12-27T00:02:49.336369Z",
     "shell.execute_reply": "2024-12-27T00:02:49.335466Z",
     "shell.execute_reply.started": "2024-12-27T00:02:49.332572Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_repeated_letters(text):\n",
    "    return bool(re.search(r'(.)\\1{2,}', str(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3837f9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:04:00.562089Z",
     "iopub.status.busy": "2024-12-27T00:04:00.561768Z",
     "iopub.status.idle": "2024-12-27T00:04:00.604917Z",
     "shell.execute_reply": "2024-12-27T00:04:00.604032Z",
     "shell.execute_reply.started": "2024-12-27T00:04:00.562065Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df[df['comment'].apply(find_repeated_letters)]\n",
    "df['comment'].head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd728f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# normalize repeated letters  \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\"   laugher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baded45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:08:34.516805Z",
     "iopub.status.busy": "2024-12-27T00:08:34.516482Z",
     "iopub.status.idle": "2024-12-27T00:08:34.523418Z",
     "shell.execute_reply": "2024-12-27T00:08:34.522657Z",
     "shell.execute_reply.started": "2024-12-27T00:08:34.516779Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to replace repeated \"Ù‡\" with \"Ø¶Ø­Ùƒ\"\n",
    "def normalize_laughter(text):\n",
    "    # Replace repeated \"Ù‡\" (3 or more times) with \"Ø¶Ø­Ùƒ\"\n",
    "    return re.sub(r'(Ù‡)\\1{2,}', 'Ø¶Ø­Ùƒ', text)\n",
    "\n",
    "# Test cases\n",
    "test_cases = [\n",
    "    \"Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡ ÙˆØ´ ÙƒØªØ´ÙˆÙ Ø´ÙŠ ÙˆØ§Ø­Ø¯ ðŸ˜‚ðŸ˜‚\",\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"Ø´Ùˆ Ù‡Ù‡Ù‡Ù‡Ù‡ Ø´Ù†Ùˆ Ø§Ù„ÙˆØ¶Ø¹\",\n",
    "    \"Ø¹Ø§Ø¯ÙŠ Ø¬Ø¯Ø§ Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡Ù‡\",\n",
    "    \"Ù‡Ø°Ø§ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¨Ø¯ÙˆÙ† Ù‡Ù‡Ù‡Ù‡Ù‡\"\n",
    "]\n",
    "\n",
    "# Apply normalization to the test cases\n",
    "normalized_test_cases = [normalize_laughter(case) for case in test_cases]\n",
    "\n",
    "# Display the original and normalized results\n",
    "for i, (original, normalized) in enumerate(zip(test_cases, normalized_test_cases)):\n",
    "    print(f\"Original Test Case {i+1}: {original}\")\n",
    "    print(f\"Normalized Test Case {i+1}: {normalized}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95507268",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:09:37.543376Z",
     "iopub.status.busy": "2024-12-27T00:09:37.543063Z",
     "iopub.status.idle": "2024-12-27T00:09:37.558383Z",
     "shell.execute_reply": "2024-12-27T00:09:37.557386Z",
     "shell.execute_reply.started": "2024-12-27T00:09:37.543350Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply the normalization to the DataFrame\n",
    "df['comment'] = df['comment'].apply(normalize_laughter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f455afb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:09:55.188486Z",
     "iopub.status.busy": "2024-12-27T00:09:55.188175Z",
     "iopub.status.idle": "2024-12-27T00:09:55.195140Z",
     "shell.execute_reply": "2024-12-27T00:09:55.194255Z",
     "shell.execute_reply.started": "2024-12-27T00:09:55.188462Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5398896b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:10:46.048594Z",
     "iopub.status.busy": "2024-12-27T00:10:46.048260Z",
     "iopub.status.idle": "2024-12-27T00:10:46.081541Z",
     "shell.execute_reply": "2024-12-27T00:10:46.080854Z",
     "shell.execute_reply.started": "2024-12-27T00:10:46.048570Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"df_after_normalisation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96afc7f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# remove_unnecessary_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc0f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:20:28.894705Z",
     "iopub.status.busy": "2024-12-27T00:20:28.894141Z",
     "iopub.status.idle": "2024-12-27T00:20:28.900684Z",
     "shell.execute_reply": "2024-12-27T00:20:28.899936Z",
     "shell.execute_reply.started": "2024-12-27T00:20:28.894673Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_unnecessary_spaces(text):\n",
    "    \"\"\"\n",
    "    Remove unnecessary spaces from the input text:\n",
    "    - Strip leading and trailing spaces.\n",
    "    - Replace multiple spaces with a single space.\n",
    "    \"\"\"\n",
    "    return re.sub(r'\\s+', ' ', text.strip())\n",
    "\n",
    "# Example usage\n",
    "example_text = \"   Ù‡Ø°Ø§   Ø§Ù„Ù†Øµ ÙŠØ­ØªÙˆÙŠ  Ø¹Ù„Ù‰  Ù…Ø³Ø§ÙØ§Øª   ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©   \"\n",
    "cleaned_text = remove_unnecessary_spaces(example_text)\n",
    "\n",
    "print(\"Original:\", example_text)\n",
    "print(\"Cleaned:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b1caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:21:04.069549Z",
     "iopub.status.busy": "2024-12-27T00:21:04.069188Z",
     "iopub.status.idle": "2024-12-27T00:21:04.129511Z",
     "shell.execute_reply": "2024-12-27T00:21:04.128667Z",
     "shell.execute_reply.started": "2024-12-27T00:21:04.069509Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(remove_unnecessary_spaces)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1cd135",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:21:59.381735Z",
     "iopub.status.busy": "2024-12-27T00:21:59.381439Z",
     "iopub.status.idle": "2024-12-27T00:21:59.411521Z",
     "shell.execute_reply": "2024-12-27T00:21:59.410863Z",
     "shell.execute_reply.started": "2024-12-27T00:21:59.381710Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"df_without_spaces.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaea632",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:28:06.565356Z",
     "iopub.status.busy": "2024-12-27T00:28:06.565029Z",
     "iopub.status.idle": "2024-12-27T00:28:07.070992Z",
     "shell.execute_reply": "2024-12-27T00:28:07.070079Z",
     "shell.execute_reply.started": "2024-12-27T00:28:06.565329Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example DataFrame\n",
    "# Assuming df is your dataset with a 'label' column for stratification\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Step 1: Shuffle the dataset\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Step 2: Stratified split into training (65%) and temp (35%)\n",
    "train_data, temp_data = train_test_split(\n",
    "    df, test_size=0.35, stratify=df['off'], random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: Stratified split of temp (35%) into validation (15%) and test (20%)\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=20/(15+20), stratify=temp_data['off'], random_state=42\n",
    ")\n",
    "\n",
    "# Print the sizes of each split\n",
    "print(f\"Training set: {len(train_data)} rows\")\n",
    "print(f\"Validation set: {len(val_data)} rows\")\n",
    "print(f\"Test set: {len(test_data)} rows\")\n",
    "\n",
    "# Optional: Save the splits to CSV files\n",
    "train_data.to_csv('train_data_cleaned.csv', index=False)\n",
    "val_data.to_csv('val_data_cleaned.csv', index=False)\n",
    "test_data.to_csv('test_data_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d6df4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:38:51.576979Z",
     "iopub.status.busy": "2024-12-27T00:38:51.576677Z",
     "iopub.status.idle": "2024-12-27T00:38:51.582823Z",
     "shell.execute_reply": "2024-12-27T00:38:51.581894Z",
     "shell.execute_reply.started": "2024-12-27T00:38:51.576952Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset class\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, texts, labels, MARBERT_tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer =MARBERT_tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898c6101",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T00:46:20.987838Z",
     "iopub.status.busy": "2024-12-27T00:46:20.987491Z",
     "iopub.status.idle": "2024-12-27T01:02:49.623161Z",
     "shell.execute_reply": "2024-12-27T01:02:49.622209Z",
     "shell.execute_reply.started": "2024-12-27T00:46:20.987801Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,  # Increased epochs with early stopping in mind\n",
    "    save_total_limit=2,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=1e-4,  # Add weight decay for regularization\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    greater_is_better=True,   \n",
    "    report_to=\"none\",                 # Disable all integrations, including W&B\n",
    ")\n",
    "\n",
    "# Prepare datasets\n",
    "train_dataset = HateSpeechDataset(train_data[\"comment\"].tolist(), train_data[\"off\"].tolist(), MARBERT_tokenizer)\n",
    "val_dataset = HateSpeechDataset(val_data[\"comment\"].tolist(), val_data[\"off\"].tolist(), MARBERT_tokenizer)\n",
    "test_dataset = HateSpeechDataset(test_data[\"comment\"].tolist(), test_data[\"off\"].tolist(), MARBERT_tokenizer)\n",
    "\n",
    "# Compute metrics\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "\n",
    "# Custom Trainer with epoch metrics tracking\n",
    "epoch_metrics = []\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def _save(self, output_dir, state_dict=None):\n",
    "        # Ensure all tensors are contiguous\n",
    "        for param in self.model.parameters():\n",
    "            if not param.is_contiguous():\n",
    "                param.data = param.data.contiguous()\n",
    "        super()._save(output_dir, state_dict)\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        # Call the original evaluation method\n",
    "        result = super().evaluate(eval_dataset=eval_dataset, ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix)\n",
    "        \n",
    "        # Add the results to the epoch metrics list\n",
    "        epoch_metric = {\n",
    "            \"epoch\": len(epoch_metrics) + 1,\n",
    "            \"loss\": result[f\"{metric_key_prefix}_loss\"],\n",
    "            \"accuracy\": result[f\"{metric_key_prefix}_accuracy\"],\n",
    "            \"precision\": result[f\"{metric_key_prefix}_precision\"],\n",
    "            \"recall\": result[f\"{metric_key_prefix}_recall\"],\n",
    "         \n",
    "            \"f1\": result[f\"{metric_key_prefix}_f1\"],\n",
    "        }\n",
    "        \n",
    "        # Add runtime if available\n",
    "        if f\"{metric_key_prefix}_runtime\" in result:\n",
    "            epoch_metric[\"runtime\"] = result[f\"{metric_key_prefix}_runtime\"]\n",
    "        \n",
    "        epoch_metrics.append(epoch_metric)\n",
    "        return result\n",
    "\n",
    "# Use the custom trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=MARBERT_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=MARBERT_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Log epoch metrics\n",
    "for metric in epoch_metrics:\n",
    "    print(metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70b656e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:21:35.497089Z",
     "iopub.status.busy": "2024-12-27T01:21:35.496711Z",
     "iopub.status.idle": "2024-12-27T01:21:36.943126Z",
     "shell.execute_reply": "2024-12-27T01:21:36.942341Z",
     "shell.execute_reply.started": "2024-12-27T01:21:35.497057Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the trained model and tokenizer\n",
    "save_directory = \"./trained_marbert_model\"\n",
    "MARBERT_model.save_pretrained(save_directory)\n",
    "MARBERT_tokenizer.save_pretrained(save_directory)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d1338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-27T01:25:32.217986Z",
     "iopub.status.busy": "2024-12-27T01:25:32.217698Z",
     "iopub.status.idle": "2024-12-27T01:26:05.189627Z",
     "shell.execute_reply": "2024-12-27T01:26:05.188514Z",
     "shell.execute_reply.started": "2024-12-27T01:25:32.217952Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Compress the directory\n",
    "shutil.make_archive(\"trained_marbert_model\", 'zip', \"./trained_marbert_model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce9afc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6352701,
     "sourceId": 10307025,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 73.740051,
   "end_time": "2024-12-27T01:30:01.029641",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-12-27T01:28:47.289590",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
